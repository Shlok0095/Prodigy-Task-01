{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "idm--TaHCzMg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hVabDkAC9vRt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from flask import Flask, request, jsonify"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preprocessing"
      ],
      "metadata": {
        "id": "FYtFzLFtDARw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Prepare your custom dataset\n",
        "def load_dataset(file_path, tokenizer, block_size=128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size,\n",
        "    )\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "LihoFXh--4f3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_path = \"/content/Title The Wandererâ€™s Path.txt.txt\"\n",
        "\n"
      ],
      "metadata": {
        "id": "lMEHYSyi-4iF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading pretrained model"
      ],
      "metadata": {
        "id": "lA8EVFefDLhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load the pre-trained GPT-2 model\n",
        "model_name = \"gpt2\"  # You can choose \"gpt2-medium\" or \"gpt2-large\" if you have more resources\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "A9gsvh0y-4j-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pad_token_id for GPT-2 tokenizer\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n"
      ],
      "metadata": {
        "id": "r4gxCVWbkvrr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Fine-tune the model\n",
        "# Prepare dataset\n",
        "train_dataset = load_dataset(train_file_path, tokenizer)\n"
      ],
      "metadata": {
        "id": "7OInHLaz-4mc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6b2598-158e-4a16-d077-8ce784683618"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False,\n",
        ")"
      ],
      "metadata": {
        "id": "zqcVtY4w-4pw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwrQYbpqhXl_",
        "outputId": "fba617f5-da66-465b-9847-b5759c5d6e37"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Hyperparameters"
      ],
      "metadata": {
        "id": "qHArZZRZDPec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_size = 4\n",
        "learning_rate = 5e-5"
      ],
      "metadata": {
        "id": "rnfPEXXWhXo_"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=data_collator)"
      ],
      "metadata": {
        "id": "X34kfrXChjW4"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "IQtwX8CWDf_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "print(\"Training the model...\")\n",
        "model.train()\n",
        "total_steps = len(dataloader) * epochs\n",
        "progress_bar = tqdm(total=total_steps, desc=\"Training\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = (input_ids != tokenizer.pad_token_id).to(device)  # Create attention mask\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        progress_bar.update(1)\n",
        "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "progress_bar.close()"
      ],
      "metadata": {
        "id": "K2VZb0Vl_O-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c89c325-89ba-4186-83ca-2ca2dfbf9d50"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:08<00:00,  6.81it/s, loss=0.0919]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Model"
      ],
      "metadata": {
        "id": "jAV1hxnCDlGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final model\n",
        "print(\"Saving the final model...\")\n",
        "model.save_pretrained(\"./fine_tuned_gpt2_final\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_gpt2_final\")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFBoOkx3iKhC",
        "outputId": "72963b80-efd0-4b6a-898b-0f810d1ca26e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the final model...\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generating from model"
      ],
      "metadata": {
        "id": "J97XTwg_DqOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Generate text\n",
        "def generate_text(prompt, model, tokenizer, max_length=100):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "VA2Aox_jDSNl"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading model"
      ],
      "metadata": {
        "id": "zQ4ZUIY5D3Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fine-tuned model\n",
        "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2_final\").to(device)\n",
        "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_gpt2_final\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IvF7c23nEBrg"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prompt Completion"
      ],
      "metadata": {
        "id": "II1OdKzVD7PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "prompt = \"kadar was nomad\"\n",
        "generated_text = generate_text(prompt, fine_tuned_model, fine_tuned_tokenizer)\n",
        "print(\"Generated text:\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akc4f0hGiH9a",
        "outputId": "babbd32d-420a-4a38-f44b-cdaede0d04cd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: kadar was nomad for many years, and was a guardian of the desert. He was wise and had seen many seasons come and go. His life was one of constant movement, following the stars at night and the whispers of wind by day.\n",
            "\n",
            "Kadara was the last of a tribe of archers, who had long since forsaken permanent settlements in favor of an ancient, life on the move. Their survival depended on their survival against the odds. They had become one with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sge_rXgzouxS"
      },
      "execution_count": 46,
      "outputs": []
    }
  ]
}